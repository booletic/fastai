Questionnaire

01  Why can't we use accuracy as a loss function?
        A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn from the gradients equal to zero, and the model’s weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better predictions. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction. The loss function needs to reflect that.
        
02  Draw the sigmoid function. What is special about its shape?
        Sigmoid function is a smooth curve that squishes all values into values between 0 and 1. Most loss functions assume that the model is outputting some form of a probability or confidence level between 0 and 1 so we use a sigmoid function at the end of the model in order to do this.
        
03  What is the difference between a loss function and a metric?
        The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.

04  What is the function to calculate new weights using a learning rate?
        The optimizer step function

05  What does the DataLoader class do?
        The DataLoader class can take any Python collection and turn it into an iterator over many batches.

06  Write pseudocode showing the basic steps taken in each epoch for SGD.
        for x,y in dl:
            pred = model(x)
            loss = loss_func(pred, y)
            loss.backward()
            parameters -= parameters.grad * lr

07  Create a function that, if passed two arguments [1,2,3,4] and 'abcd', returns [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]. What is special about that output data structure?
        def func(a,b): return list(zip(a,b))
        This data structure is useful for machine learning models when you need lists of tuples where each tuple would contain input data and a label.

08  What does view do in PyTorch?
        It changes the shape of a Tensor without changing its contents.

09  What are the "bias" parameters in a neural network? Why do we need them?
        Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.
        
10  What does the @ operator do in Python?
        This is the matrix multiplication operator.

11  What does the backward method do?
        This method returns the current gradients.

12  Why do we have to zero the gradients?
        PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value.
        
13  What information do we have to pass to Learner?
        We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print.
        
14  Show Python or pseudocode for the basic steps of a training loop.
        def train_epoch(model, lr, params):
            for xb,yb in dl:
                calc_grad(xb, yb, model)
                for p in params:
                    p.data -= p.grad*lr
                    p.grad.zero_()
        for i in range(20):
            train_epoch(model, lr, params)
            
15  What is "ReLU"? Draw a plot of it for values from -2 to +2.
        ReLU just means “replace any negative numbers with zero”. It is a commonly used activation function.

16  What is an "activation function"?
        The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem.

17  What's the difference between F.relu and nn.ReLU?
        F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu.

18  The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?
        There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.

19  Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU?
        This concept is known as presizing. Data augmentation is often applied to the images and in fastai it is done on the GPU. However, data augmentation can lead to degradation and artifacts, especially at the edges. Therefore, to minimize data destruction, the augmentations are done on a larger image, and then RandomResizeCrop is performed to resize to the final image size.
        
20  If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book's website for suggestions.
        To be done by the reader.

21  What are the two ways in which data is most commonly provided, for most deep learning datasets?
        Individual files representing items of data, such as text documents or images.
        A table of data, such as in CSV format, where each row is an item, each row which may include filenames providing a connection between the data in the table and data in other formats such as text documents and images.

22  Look up the documentation for L and try using a few of the new methods is that it adds.
        To be done by the reader.

23  Look up the documentation for the Python pathlib module and try using a few methods of the Path class.
        To be done by the reader.

24  Give two examples of ways that image transformations can degrade the quality of the data.
        Rotation can leave empty areas in the final image
        Other operations may require interpolation which is based on the original image pixels but are still of lower image quality

25  What method does fastai provide to view the data in a DataLoaders?
        DataLoader.show_batch

26  What method does fastai provide to help you debug a DataBlock?
        DataBlock.summary

27  Should you hold off on training a model until you have thoroughly cleaned your data?
        No. It is best to create a baseline model as soon as possible.

28  What are the two pieces that are combined into cross-entropy loss in PyTorch?
        Cross Entropy Loss is a combination of a Softmax function and Negative Log Likelihood Loss.

29  What are the two properties of activations that softmax ensures? Why is this important?
        It makes the outputs for the classes add up to one. This means the model can only predict one class. Additionally, it amplifies small changes in the output activations, which is helpful as it means the model will select a label with higher confidence (good for problems with definite labels)

30  When might you want your activations to not have these two properties?
        When you have multi-label classification problems (more than one label possible).

31  Calculate the exp and softmax columns of <<bear_softmax>> yourself (i.e., in a spreadsheet, with a calculator, or in a notebook).
        To be done by the reader.
